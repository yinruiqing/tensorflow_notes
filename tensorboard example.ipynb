{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard可视化\n",
    "我们使用了之前的线性回归的例子，来展示tensorboard 的用法。具体的说明可以参考官方文档，当然也可以参考中文版的。\n",
    "\n",
    "我们可以对关心的节点添加summary 操作（如果该节点对应的tensor里面有多个元素，一般使用histogram_summary来看它们的分布。比如说权重矩阵。如果该节点对应的是一个标量，如loss值，那么我们就可以用summary.scalar）\n",
    "\n",
    "在TensorFlow中，所有的操作只有当你执行，或者另一个操作依赖于它的输出时才会运行。我们刚才创建的这些summary节点都围绕着你的图像：没有任何操作依赖于它们的结果。因此，为了生成汇总信息，我们需要运行所有这些节点。我们可以使用tf.merge_all_summaries来将他们合并为一个操作。\n",
    "\n",
    "然后你可以执行合并命令，它会依据特点步骤将所有数据生成一个序列化的Summary protobuf对象。最后，为了将汇总数据写入磁盘，需要将汇总的protobuf对象传递给tf.train.Summarywriter。\n",
    "\n",
    "SummaryWriter 的构造函数中包含了参数 logdir。这个 logdir 非常重要，所有事件都会写到它所指的目录下。此外，SummaryWriter 中还包含了一个可选择的参数 GraphDef。如果输入了该参数，那么 TensorBoard 也会显示你的图像。\n",
    "\n",
    "另外非常重要的一点，典型的 TensorFlow 计算图有数以千计的节点，为简单起见，我们为节点对应的变量名划定范围，这样就会产生一个层级结构。默认情况下， 只有顶层节点会显示（该处不明白）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.linspace(-1, 1, 300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, n_layer, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    layer_name = 'layer%s' % n_layer\n",
    "\n",
    "    # for tensorflow version < 0.12\n",
    "    if int((tf.__version__).split('.')[1]) < 12:\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('weights'):\n",
    "                Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')\n",
    "                tf.histogram_summary(layer_name + '/weights', Weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')\n",
    "                tf.histogram_summary(layer_name + '/biases', biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "            if activation_function is None:\n",
    "                outputs = Wx_plus_b\n",
    "            else:\n",
    "                outputs = activation_function(Wx_plus_b, )\n",
    "            tf.histogram_summary(layer_name + '/outputs', outputs)\n",
    "    else:  \n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('weights'):\n",
    "                Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')\n",
    "                tf.summary.histogram(layer_name + '/weights', Weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')\n",
    "                tf.summary.histogram(layer_name + '/biases', biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "            if activation_function is None:\n",
    "                outputs = Wx_plus_b\n",
    "            else:\n",
    "                outputs = activation_function(Wx_plus_b, )\n",
    "            tf.summary.histogram(layer_name + '/outputs', outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')\n",
    "    ys = tf.placeholder(tf.float32, [None, 1], name='y_input')\n",
    "\n",
    "l1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)\n",
    "prediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None)\n",
    "\n",
    "# tensorflow 在版本0.12之后，一些函数发生了变化，所以针对不同的版本我们会使用不同的summary函数\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                                        reduction_indices=[1]))\n",
    "    if int((tf.__version__).split('.')[1]) < 12:\n",
    "        tf.scalar_summary('loss', loss)\n",
    "    else:   \n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "if int((tf.__version__).split('.')[1]) < 12:  \n",
    "    merged = tf.merge_all_summaries()\n",
    "else:  \n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "#把summary内容放到文件夹\"logs/\"里 \n",
    "if int((tf.__version__).split('.')[1]) < 12:\n",
    "    writer = tf.train.SummaryWriter('logs/', sess.graph)\n",
    "else: \n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "\n",
    "if int((tf.__version__).split('.')[1]) < 12:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 接着打开terminal，进入你存放的文件夹地址上一层，运行命令 tensorboard --logdir='logs/'\n",
    "# 会返回一个地址，然后用浏览器打开这个地址，在 graph 标签栏下打开\n",
    "for i in range(1000):\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 50 == 0:\n",
    "        #每50次将summary统计一次\n",
    "        result = sess.run(merged,\n",
    "                          feed_dict={xs: x_data, ys: y_data})\n",
    "        writer.add_summary(result, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着打开terminal，进入你存放的文件夹地址上一层，运行命令 tensorboard --logdir='logs/'\n",
    "会返回一个地址，然后用浏览器打开这个地址，在 graph 标签栏下打开。记得一定要选择装了tensorflow的python环境。\n",
    "<img src=\"imgs/cmd_tensorboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开浏览器输入地址看到的内容：\n",
    "<img src=\"imgs/emp_tensorboard.png\">\n",
    "从图中我们可以很明显的看到变量的层级结构。layer1下的biais，output和weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
